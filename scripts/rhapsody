#!/usr/bin/env python3
import os
import time
import click
import datetime
from decimal import Decimal
from tqdm import tqdm
import pandas as pd
import numpy as np
from biom import load_table, Table
from biom.util import biom_open
from skbio.stats.composition import clr, centralize, closure
from skbio.stats.composition import clr_inv as softmax
from scipy.stats import entropy, spearmanr
from scipy.sparse import csr_matrix
from rhapsody.mmvec import MMvec
from rhapsody.util import (onehot, rank_hits, random_multimodal,
                           split_tables, format_params)


@click.group()
def rhapsody():
    pass


@rhapsody.command()
@click.option('--input-abundances',
              help=('Input abundances to use for prediction. '
                    'These could be metabolite intensities or sequence counts.'
              ))
@click.option('--output-abundances',
              help=('Output abundances to predict from input abundances. '
                    'These could be metabolite intensities or sequence counts.'
              ))
@click.option('--metadata-file', default=None,
              help='Input sample metadata file')
@click.option('--training-column',
              help=('Column in the sample metadata specifying which '
                    'samples are for training and testing.'),
              default=None)
@click.option('--num-testing-examples',
              help=('Number of samples to randomly select for testing'),
              default=10)
@click.option('--min-feature-count',
              help=('Minimum number of samples a microbe needs to be observed '
                    'in order to not filter out'),
              default=10)
@click.option('--epochs',
              help=('Number of epochs to train, one epoch represents the '
                    'number of samples to process an entire dataset.'), default=10)
@click.option('--batch-size',
              help='Number of samples to analyze per iteration.', default=10)
@click.option('--subsample-size',
              help='Number of sequences to analyze per sample.', default=100)
@click.option('--mc-samples',
              help='Number of Monte Carlo samples to estimate gradient steps.',
              default=5)
@click.option('--latent-dim',
              help=('Dimensionality of shared latent space. '
                    'This is analogous to the number of PC axes.'),
              default=3)
@click.option('--arm-the-gpu', is_flag=True,
              help=('Enables GPU support'),
              default=False)
@click.option('--learning-rate',
              help=('Gradient descent learning rate.'),
              default=1e-1)
@click.option('--beta1',
              help=('Gradient decay rate for first Adam momentum estimates'),
              default=0.9)
@click.option('--beta2',
              help=('Gradient decay rate for second Adam momentum estimates'),
              default=0.95)
@click.option('--seed',
              help=('Random seed for the sake of reproducibility (optional).'),
              default=None)
@click.option('--checkpoint-interval',
              help=('Number of seconds before a storing a checkpoint.'),
              default=1000)
@click.option('--summary-interval',
              help=('Number of seconds before a storing a summary.'),
              default=1000)
@click.option('--summary-dir', default='summarydir',
              help='Summary directory to save cross validation results.')
@click.option('--embeddings-file', default=None,
              help=('Path to save the embeddings learned from the model. '
                    'If this is not specified, then this will be saved under '
                    '`--summary-dir`.'))
@click.option('--ranks-file', default=None,
              help=('Path to save the ranks learned from the model. '
                    'If this is not specified, then this will be saved under '
                    '`--summary-dir`.'))
@click.option('--ordination-file', default=None,
              help=('Path to save the ordination learned from the model. '
                    'If this is not specified, then this will be saved under '
                    '`--summary-dir`.'))
def mmvec(input_abundances, output_abundances,
          metadata_file, training_column,
          num_testing_examples, min_feature_count,
          epochs, batch_size, subsample_size, mc_samples,
          latent_dim, arm_the_gpu, learning_rate,
          beta1, beta2, seed,
          checkpoint_interval, summary_interval,summary_dir,
          embeddings_file, ranks_file, ordination_file):
    # set random seed if specified
    if seed is not None:
        np.random.seed(seed)
        torch.manual_seed(seed)

    # TODO: rename from microbes/metabolites to something more general
    microbes = load_table(input_abundances)
    metabolites = load_table(ouput_abundances)

    if metadata_file is not None:
        metadata = pd.read_table(metadata_file, index_col=0)
    else:
        metadata = None

    res = split_tables(
        microbes, metabolites,
        metadata=metadata, training_column=training_column,
        num_test=num_testing_examples,
        min_samples=min_feature_count)

    (train_microbes_df, test_microbes_df,
     train_metabolites_df, test_metabolites_df) = res

    # filter out low abundance microbes
    microbe_ids = microbes.ids(axis='observation')
    metabolite_ids = metabolites.ids(axis='observation')

    params = []

    sname = '_'.join(['log_PC(%d)' % latent_dim,
                      'samp(%d)' % batch_size,
                      'sub(%d)' % subsample_size,
                      'lr(%s)' % "{:.2E}".format(Decimal(learning_rate)),
                      'beta1(%.2f)' % beta1,
                      'beta2(%.2f)' % beta2])

    sname = os.path.join(summary_dir, sname)

    trainX = csr_matrix(train_microbes_df.values)
    testX = csr_matrix(test_microbes_df.values)
    trainY = train_metabolites_df.values
    testY = test_metabolites_df.values

    n, d1 = trainX.shape
    n, d2 = trainY.shape

    if arm_the_gpu:
        # pick out the first GPU
        device_name='cuda'
    else:
        device_name='cpu'

    model = MMvec(num_microbes=d1, num_metabolites=d2, latent_dim=latent_dim,
                  batch_size=batch_size, subsample_size=subsample_size,
                  device=device_name)
    model.fit(trainX, trainY, testX, testY,
              epochs=epochs,  learning_rate=learning_rate,
              beta1=beta1, beta2=beta2, step_size=step_size,
              summary_interval=summary_interval,
              checkpoint_interval=checkpoint_interval)

    # Save to an embeddings file
    embeds = model.embeddings(train_microbes_df.columns,
                              train_metabolites_df.columns)
    embeds.to_csv(embeddings_file)

    # Save to ranks file
    ranks = model.ranks()
    ranks.to_csv(ranks_file)

    # Save to ordination file
    ordination = model.ordination(train_microbes_df.columns,
                                  train_metabolites_df.columns)
    ordination.write(ordination_file)


if __name__ == '__main__':
    rhapsody()
